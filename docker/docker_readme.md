# Python Code Execution Sandbox

This component provides a secure, isolated sandbox for executing Python code generated by a Large Language Model (LLM). It runs as a lightweight microservice in a Docker container, exposing a simple API endpoint to receive and run code.

The primary goal is to provide a "tool" that an LLM agent can use to perform calculations, generate plots, or analyze data without compromising the host system.

## Features

- **Secure by Design:** Code is executed inside a container with a non-root user and has no access to the host filesystem.
- **RESTful API:** A simple `/execute` endpoint receives code via a POST request and returns `stdout`, `stderr`, and any generated images in a structured JSON format.
- **Pre-loaded Libraries:** Comes with common data science and utility libraries pre-installed (`numpy`, `pandas`, `matplotlib`, `scikit-learn`, `requests`).
- **Plot & Image Capture:** Automatically captures `matplotlib` plots and returns them as Base64-encoded strings, ready for display or saving.
- **Lightweight:** Based on a slim Debian image to keep resource usage minimal.

## Prerequisites

- [Docker](https://www.docker.com/get-started) must be installed and the Docker daemon must be running.
- On Linux, your user should be part of the `docker` group to run commands without `sudo`.
- On the RPi, installing via apt works just fine (sudo apt install docker.io)

## Directory Contents

- **`Dockerfile`**: The recipe for building the Docker image. It sets up the environment, installs dependencies, and configures the container.
- **`app.py`**: A small Flask web server that provides the `/execute` API endpoint and handles the code execution logic.
- **`requirements.txt`**: A list of Python libraries to be installed inside the container.
- **`agent.py`**: A standalone command-line script for testing the sandbox with a two-LLM agentic workflow (conversation and coding).

---

## Build and Installation

These commands should be run from the **root of the `jn-66` project directory**, not from inside the `docker/` folder.

### 1. Build the Docker Image

The `docker build` command reads the `Dockerfile`, installs all dependencies, and creates a reusable image named `llm-sandbox`.

``` bash
# The path './docker' tells Docker where to find the Dockerfile and its context.
docker build -t llm-sandbox ./docker
```

This process may take a few minutes the first time as it downloads the base image and installs the Python packages. Subsequent builds will be much faster due to Docker's caching.
### 2. Run the Sandbox Container

Once the image is built, you can run it as a persistent, background service.

``` bash
docker run \
    -d \
    --name=code-executor \
    --restart unless-stopped \
    -p 5000:5000 \
    llm-sandbox
```

Let's break down these flags:

- -d: Detached Mode. Runs the container in the background.
- --name=code-executor: Gives the container a convenient, stable name.
- --restart unless-stopped: Highly Recommended. Automatically restarts the container if it crashes or if the system reboots.
- -p 5000:5000: Port Mapping. Maps port 5000 on your host machine to port 5000 inside the container, making the API accessible at http://localhost:5000.

### 3. Verify the Sandbox is Running

You can check the status of your container at any time with:

``` bash
docker ps
```
You should see code-executor in the list with a status of "Up". To see its logs, you can run:

``` bash
docker logs code-executor
```
## How to Use and Test the Sandbox

The sandbox is now running and listening for requests on http://localhost:5000/execute.

### Testing with curl

You can send code directly to the endpoint using curl from your terminal.

Example 1: Simple text output

``` bash
curl -X POST -H "Content-Type: application/json" \
     -d '{"code": "import numpy as np; print(np.sqrt(16))"}' \
     http://localhost:5000/execute
```
Expected Response: {"image_b64":null,"stderr":"","stdout":"4.0\n"}

Example 2: Generating a plot

``` bash
curl -X POST -H "Content-Type: application/json" \
     -d '{"code": "import matplotlib.pyplot as plt; plt.plot();"}' \
     http://localhost:5000/execute
```

Expected Response: A JSON object where image_b64 is populated with a long Base64 string representing the PNG image.

### Testing with the Standalone Agent

The included agent.py script provides a full command-line interface to test the sandbox with a "Reason and Act" LLM workflow.

To run it:

``` bash
# Make sure you have the required libraries installed locally first
pip install ollama requests Pillow

# Run the agent
python docker/agent.py
```

You can then interact with the agent, which will call the sandbox API when it decides to run code.

### Stopping the Sandbox
If you need to stop the container, you can do so with:

``` bash
docker stop code-executor
```
If you started it with --restart unless-stopped, it will stay stopped. To start it again, simply use docker start code-executor.